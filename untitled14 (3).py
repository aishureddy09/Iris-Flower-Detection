# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pY0Wj6FfpRrBBfjh3VDWS9e9YAmr6a-h
"""

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
data = pd.read_csv("/content/Advertising.csv.csv",index_col=0)
data


# In[2]:


data.info()


# In[3]:


import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15,6))
sns.lineplot(data=data,x=data['TV'],y=data['sales'])


# In[4]:


plt.figure(figsize=(15,6))
sns.lineplot(data=data,x=data['Radio'],y=data['sales'])


# In[7]:


plt.figure(figsize=(15,6))
sns.lineplot(data=data,x=data['newspaper'],y=data['sales'])


# In[9]:


data.describe()


# In[23]:


x = data.drop(['TV','Radio','newspaper'],axis=1)
print(x)
y = data['sales']


# In[27]:



plt.title("sales Rate")
plt.xlabel("Products")
plt.ylabel("sales")
plt.show()


# In[18]:


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=0)


# In[19]:


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=0)


# In[29]:


from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor()


# In[30]:


rfr.fit(x_train,y_train)


# In[31]:


y_pred = rfr.predict(x_test)


# In[32]:


accuracy = rfr.score(x_test,y_test)
print("Accuracy:",accuracy*100)


# In[ ]:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression


# In[2]:


data = pd.read_csv("/content/Unemployment.csv.csv")
data


# In[3]:


data = data.dropna()
data.info()


# In[4]:


data[' Date'] = pd.to_datetime(data[' Date'])
data.describe()


# In[5]:


min = data[' Date'].min()
max = data[' Date'].max()
print(min,max)


# In[6]:


plt.figure(figsize=(15,5))
plt.plot(data[' Date'], data[' Estimated Unemployment Rate (%)'])
plt.xlabel('Date')
plt.ylabel('Unemployment Rate')
plt.title('Unemployment Rate in India')
plt.show()


# In[7]:


plt.figure(figsize=(15,5))
sns.lineplot(x=' Date',y = ' Estimated Unemployment Rate (%)',data=data)


# In[8]:


mean_unemployment = data[' Estimated Unemployment Rate (%)'].mean()
median_unemployment = data[' Estimated Unemployment Rate (%)'].median()
print(f'Mean Unemployment Rate: {mean_unemployment}')
print(f'Median Unemployment Rate: {median_unemployment}')


# In[10]:


rate = pd.read_csv("Desktop/Unemployment01.csv")
rate.info()


# In[11]:


plt.figure(figsize=(15,5))
plt.plot(rate[' Date'],rate[' Estimated Unemployment Rate (%)'])
plt.xlabel("Date")
plt.ylabel("Unemployment Rate")
plt.title("Unemployment rate Upto 11-2020")
plt.show()


# In[12]:


plt.figure(figsize=(15,5))
sns.lineplot(x=' Date',y = ' Estimated Unemployment Rate (%)',data=rate)


# In[13]:


mean_unemployment = rate[' Estimated Unemployment Rate (%)'].mean()
median_unemployment = rate[' Estimated Unemployment Rate (%)'].median()
print(f'Mean Unemployment Rate during Covid-19: {mean_unemployment}')
print(f'Median Unemployment Rate during Covid-19: {median_unemployment}')


# In[ ]:

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import GridSearchCV


# In[5]:


email = pd.read_csv("/content/Email.csv.xlsx")(encoding ='ISO-8859-1')
email


# In[6]:


email = email.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)
email = email.dropna()
email.info()


# In[7]:


X = email['v2'].values
y = email['v1'].values


# In[9]:


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)


# In[10]:


cv = CountVectorizer()
X_train = cv.fit_transform(X_train)
X_test = cv.transform(X_test)


# In[11]:


from sklearn.svm import SVC
svm = SVC(kernel = 'rbf', random_state=0)
svm.fit(X_train,y_train)


# In[12]:


y_pred = svm.predict(X_test)


# In[13]:


from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test,y_pred)
print("Accuracy:",accuracy*100)


# In[ ]:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# In[15]:


columns = ['SpealLengthCm','SpealWidthCm','PetalLengthCm','PetalWidthCm','Species']
data = pd.read_csv('Desktop/iris.csv',index_col=0)


# In[16]:


data


# In[17]:


data.describe()


# In[18]:


sns.pairplot(data, hue='Species')


# In[19]:


data = data.values
X = data[:,0:4]
Y = data[:,4]


# In[20]:


# Calculate average of each features for all classes
Y_Data = np.array([np.average(X[:, i][Y==j].astype('float32')) 
for i in range (X.shape[1])
 for j in (np.unique(Y))])
Y_Data_reshaped = Y_Data.reshape(4, 3)
Y_Data_reshaped = np.swapaxes(Y_Data_reshaped, 0, 1)
X_axis = np.arange(len(columns)-1)
width = 0.25


# In[21]:


# Plot the average
plt.bar(X_axis, Y_Data_reshaped[0], width, label = 'Setosa')
plt.bar(X_axis+width, Y_Data_reshaped[1], width, label = 'Versicolour')
plt.bar(X_axis+width*2, Y_Data_reshaped[2], width, label = 'Virginica')
plt.xticks(X_axis, columns[:4])
plt.xlabel("Features")
plt.ylabel("Value in cm.")
plt.legend(bbox_to_anchor=(1.3,1))
plt.show()


# In[22]:


# Split the data to train and test dataset.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)


# In[23]:


# Support vector machine algorithm
from sklearn.svm import SVC
svn = SVC()
svn.fit(X_train, y_train)


# In[25]:


# Predict from the test dataset
predictions = svn.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:",accuracy*100)


# In[26]:


# classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))


# In[29]:


X_new = np.array([[3, 2, 1, 0.2], [  4.9, 2.2, 3.8, 1.1 ], [  5.3, 2.5, 4.6, 1.9 ]])
prediction = svn.predict(X_new)
print("Prediction of Species: {}".format(prediction))


# In[ ]: